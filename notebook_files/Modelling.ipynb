{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"output_files/combined.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['REPORTED_TERM'] = df_train['REPORTED_TERM'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ART_CODE</th>\n",
       "      <th>DESC_CODED</th>\n",
       "      <th>HLGT_NAME_COMPL</th>\n",
       "      <th>HLT_NAME_COMPL</th>\n",
       "      <th>INC_CODE</th>\n",
       "      <th>INC_CODE_J</th>\n",
       "      <th>LLT_NAME_COMPL</th>\n",
       "      <th>PT_NAME_COMPL</th>\n",
       "      <th>REPORTED_TERM</th>\n",
       "      <th>SOC_CODE</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hyponatraemia</td>\n",
       "      <td>ELECTROLYTE AND FLUID BALANCE CONDITIONS</td>\n",
       "      <td>SODIUM IMBALANCE</td>\n",
       "      <td>10021038.0</td>\n",
       "      <td>10021036</td>\n",
       "      <td>HYPONATREMIA</td>\n",
       "      <td>HYPONATRAEMIA</td>\n",
       "      <td>hyponatremia</td>\n",
       "      <td>10027433.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Subacute cutaneous lupus erythematosus</td>\n",
       "      <td>EPIDERMAL AND DERMAL CONDITIONS</td>\n",
       "      <td>CONNECTIVE TISSUE DISORDERS</td>\n",
       "      <td>10057903.0</td>\n",
       "      <td>10057903</td>\n",
       "      <td>SUBACUTE CUTANEOUS LUPUS ERYTHEMATOSUS</td>\n",
       "      <td>SUBACUTE CUTANEOUS LUPUS ERYTHEMATOSUS</td>\n",
       "      <td>omeprazole induced subacute cutaneous lupus er...</td>\n",
       "      <td>10040785.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Blood bilirubin unconjugated increased</td>\n",
       "      <td>HEPATOBILIARY INVESTIGATIONS</td>\n",
       "      <td>LIVER FUNCTION ANALYSES</td>\n",
       "      <td>10021709.0</td>\n",
       "      <td>10021709</td>\n",
       "      <td>INDIRECT BILIRUBIN INCREASED</td>\n",
       "      <td>BLOOD BILIRUBIN UNCONJUGATED INCREASED</td>\n",
       "      <td>indirect bilirubin (74.7 micromol/l)</td>\n",
       "      <td>10022891.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>toxic epidermal necrolysis</td>\n",
       "      <td>EPIDERMAL AND DERMAL CONDITIONS</td>\n",
       "      <td>BULLOUS CONDITIONS</td>\n",
       "      <td>10044223.0</td>\n",
       "      <td>10044223</td>\n",
       "      <td>TOXIC EPIDERMAL NECROLYSIS</td>\n",
       "      <td>TOXIC EPIDERMAL NECROLYSIS</td>\n",
       "      <td>toxic epidermal necrolysis</td>\n",
       "      <td>10040785.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Bradycardia</td>\n",
       "      <td>CARDIAC ARRHYTHMIAS</td>\n",
       "      <td>RATE AND RHYTHM DISORDERS NEC</td>\n",
       "      <td>10006093.0</td>\n",
       "      <td>10006093</td>\n",
       "      <td>BRADYCARDIA</td>\n",
       "      <td>BRADYCARDIA</td>\n",
       "      <td>bradycardia</td>\n",
       "      <td>10007541.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ART_CODE                              DESC_CODED  \\\n",
       "0         0                           Hyponatraemia   \n",
       "1         1  Subacute cutaneous lupus erythematosus   \n",
       "2         2  Blood bilirubin unconjugated increased   \n",
       "3         3              toxic epidermal necrolysis   \n",
       "4         4                             Bradycardia   \n",
       "\n",
       "                            HLGT_NAME_COMPL                 HLT_NAME_COMPL  \\\n",
       "0  ELECTROLYTE AND FLUID BALANCE CONDITIONS               SODIUM IMBALANCE   \n",
       "1           EPIDERMAL AND DERMAL CONDITIONS    CONNECTIVE TISSUE DISORDERS   \n",
       "2              HEPATOBILIARY INVESTIGATIONS        LIVER FUNCTION ANALYSES   \n",
       "3           EPIDERMAL AND DERMAL CONDITIONS             BULLOUS CONDITIONS   \n",
       "4                       CARDIAC ARRHYTHMIAS  RATE AND RHYTHM DISORDERS NEC   \n",
       "\n",
       "     INC_CODE  INC_CODE_J                          LLT_NAME_COMPL  \\\n",
       "0  10021038.0    10021036                            HYPONATREMIA   \n",
       "1  10057903.0    10057903  SUBACUTE CUTANEOUS LUPUS ERYTHEMATOSUS   \n",
       "2  10021709.0    10021709            INDIRECT BILIRUBIN INCREASED   \n",
       "3  10044223.0    10044223              TOXIC EPIDERMAL NECROLYSIS   \n",
       "4  10006093.0    10006093                             BRADYCARDIA   \n",
       "\n",
       "                            PT_NAME_COMPL  \\\n",
       "0                           HYPONATRAEMIA   \n",
       "1  SUBACUTE CUTANEOUS LUPUS ERYTHEMATOSUS   \n",
       "2  BLOOD BILIRUBIN UNCONJUGATED INCREASED   \n",
       "3              TOXIC EPIDERMAL NECROLYSIS   \n",
       "4                             BRADYCARDIA   \n",
       "\n",
       "                                       REPORTED_TERM    SOC_CODE  len  \n",
       "0                                       hyponatremia  10027433.0    1  \n",
       "1  omeprazole induced subacute cutaneous lupus er...  10040785.0    4  \n",
       "2               indirect bilirubin (74.7 micromol/l)  10022891.0    4  \n",
       "3                         toxic epidermal necrolysis  10040785.0    3  \n",
       "4                                        bradycardia  10007541.0    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_list = list(df_train['REPORTED_TERM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reported_terms_ints = []\n",
    "for term in terms_list:\n",
    "    reported_terms_ints.append([vocab_to_int[word] for word in term.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier review stats\n",
    "reported_term_lens = Counter([len(x) for x in reported_terms_ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 43\n"
     ]
    }
   ],
   "source": [
    "print(\"Zero-length reviews: {}\".format(reported_term_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(reported_term_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63233"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reported_terms_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reported_terms_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reported_terms_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reported_terms_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "\n",
    "features = pad_features(reported_terms_ints, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train.ART_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = np.array(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(50586, 40) \n",
      "Validation set: \t(6323, 40) \n",
      "Test set: \t\t(6324, 40)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 40])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ...,     0, 25002,  1809],\n",
      "        [    0,     0,     0,  ...,     4,  6939,   109],\n",
      "        [    0,     0,     0,  ...,     0,     0,   115],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,   548,  2038,  2303],\n",
      "        [    0,     0,     0,  ...,   260,  5149,    75],\n",
      "        [    0,     0,     0,  ...,     4,    16,   222]])\n",
      "\n",
      "Sample label size:  torch.Size([50, 5016])\n",
      "Sample label: \n",
      " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(valid_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # stack up lstm outputs\n",
    "        #lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.softmax(out)\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = encoded_labels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(26778, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=5016, bias=True)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = output_size\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/anaconda3/envs/deeplearning/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/rahul/anaconda3/envs/deeplearning/lib/python3.7/site-packages/ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Input Shape Issue:,inputs.shape\n",
      "Epoch: 1/30... Step: 100... Loss: 7.891144... Val Loss: 7.964530\n",
      "Validation - Input Shape Issue:,inputs.shape\n",
      "Epoch: 1/30... Step: 200... Loss: 7.865648... Val Loss: 7.961387\n",
      "Validation - Input Shape Issue:,inputs.shape\n",
      "Epoch: 1/30... Step: 300... Loss: 7.998343... Val Loss: 7.958833\n",
      "Validation - Input Shape Issue:,inputs.shape\n",
      "Epoch: 1/30... Step: 400... Loss: 8.080306... Val Loss: 7.955067\n",
      "Validation - Input Shape Issue:,inputs.shape\n",
      "Epoch: 1/30... Step: 500... Loss: 7.929850... Val Loss: 7.952180\n",
      "Validation - Input Shape Issue:,inputs.shape\n",
      "Epoch: 1/30... Step: 600... Loss: 8.119182... Val Loss: 7.948170\n",
      "Validation - Input Shape Issue:,inputs.shape\n",
      "Epoch: 1/30... Step: 700... Loss: 8.060533... Val Loss: 7.944318\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 30 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        # get the output from the model\n",
    "        if((inputs.shape[0],inputs.shape[1]) != (batch_size, seq_length)):\n",
    "            print('Validation - Input Shape Issue:,inputs.shape')\n",
    "            continue\n",
    "        output, h = net(inputs, h)\n",
    "        # calculate the loss and perform backprop\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        loss = criterion(output, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                \n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                if((inputs.shape[0],inputs.shape[1]) != (batch_size, seq_length)):\n",
    "                    print('Validation - Input Shape Issue:,inputs.shape')\n",
    "                    continue\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                labels = torch.tensor(labels, dtype=torch.long)\n",
    "                #loss = criterion(output, torch.max(labels, 1)[1])\n",
    "                val_loss = criterion(output, torch.max(labels, 1)[1])\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
